{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "get_ipython().magic(u'matplotlib inline')\n",
    "%matplotlib tk\n",
    "# tk needed for animation (could differ on Windows)\n",
    "import mne\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from scipy import signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading EGI header from /home/akkus/Documents/NumLab/EEG/Megosztott EEG/test1_data/Fanni 20110317 1104002.raw...\n",
      "    Reading events ...\n",
      "    Assembling measurement info ...\n",
      "    Synthesizing trigger channel \"STI 014\" ...\n",
      "    Excluding events {} ...\n",
      "Reading EGI header from /home/akkus/Documents/NumLab/EEG/Megosztott EEG/test1_data/Fanni 20110317 1104004.raw...\n",
      "    Reading events ...\n",
      "    Assembling measurement info ...\n",
      "    Synthesizing trigger channel \"STI 014\" ...\n",
      "    Excluding events {} ...\n"
     ]
    }
   ],
   "source": [
    "### Read file ###\n",
    "path = '/home/akkus/Documents/NumLab/EEG/Megosztott EEG/test1_data/'\n",
    "raw0 = mne.io.read_raw_egi(path + 'Fanni 20110317 1104002.raw')\n",
    "raw1 = mne.io.read_raw_egi(path + 'Fanni 20110317 1104004.raw')\n",
    "# raw.plot(duration=60, n_channels=128, remove_dc=False)\n",
    "\n",
    "# Set montage\n",
    "# TODO check if this montage is appropriate for our data\n",
    "raw0.rename_channels(dict(zip(raw0.ch_names, [ch_name.replace('EG ', '').replace('E0', 'E').replace('E0', 'E') for ch_name in raw0.ch_names])))\n",
    "raw1.rename_channels(dict(zip(raw1.ch_names, [ch_name.replace('EG ', '').replace('E0', 'E').replace('E0', 'E') for ch_name in raw1.ch_names])))\n",
    "mont = mne.channels.read_montage(kind='GSN-HydroCel-128')\n",
    "raw0.set_montage(mont)\n",
    "raw1.set_montage(mont)\n",
    "# mne.viz.plot_montage(mont, show_names=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "320 events found\n",
      "Events id: [1]\n",
      "320 events found\n",
      "Events id: [1]\n"
     ]
    }
   ],
   "source": [
    "### Find events in raw data ###\n",
    "events0 = mne.find_events(raw0, stim_channel='stim')  # 'STI 014' 'resp' and 'stim' channels include events\n",
    "events1 = mne.find_events(raw1, stim_channel='stim')\n",
    "# events = mne.find_events(raw, stim_channel='resp') # for response locked epochs\n",
    "# Recode events from log file\n",
    "log_data0 = pd.read_csv(path + 'Fanni 20110317 1104_2.csv', header=None, names=['onset', 'session', 'stim-resp', 'event_type', 'nothing'])\n",
    "log_data1 = pd.read_csv(path + 'Fanni 20110317 1104_4.csv', header=None, names=['onset', 'session', 'stim-resp', 'event_type', 'nothing'])\n",
    "# print log_data\n",
    "events0[:, 2] = log_data0[log_data0['event_type']=='stim']['stim-resp'] # for stim locked epochs\n",
    "events1[:, 2] = log_data1[log_data1['event_type']=='stim']['stim-resp']\n",
    "# events[:, 2] = log_data[log_data['event_type']=='resp']['stim-resp'].replace(to_replace=['p', 'q'], value=[1, 2])  # for response locked epochs\n",
    "#print events\n",
    "# mne.viz.plot_events(events)\n",
    "# mne.viz.plot_events(events2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Deleting the first two and the last events from each block (to give relatively clean data to the ICA algorithm)\n",
    "raw0.crop(tmin=(events0[2][0]-200)/1000, tmax=(events0[len(events0)-1][0])/1000, copy=False)\n",
    "raw1.crop(tmin=(events1[2][0]-200)/1000, tmax=(events1[len(events1)-1][0])/1000, copy=False)\n",
    "\n",
    "events0 = np.delete(events0, [0,1,len(events0)-1], 0)\n",
    "events1 = np.delete(events1, [0,1,len(events1)-1], 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Concatenating raw files\n",
    "raw, events = mne.concatenate_raws([raw0, raw1], events_list=[events0, events1])\n",
    "del raw0, raw1, events0, events1, log_data0, log_data1\n",
    "# TODO Maybe the raw files should be handled separately until Stage III is completed (ICA)? This possibly depends on whether an adjustment was made by hand between blocks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading 0 ... 883001  =      0.000 ...   883.001 secs...\n",
      "Applying a custom EEG reference.\n"
     ]
    }
   ],
   "source": [
    "### Setting Fz as reference ###\n",
    "raw.load_data()\n",
    "fz_ch = ['E11'] # According to the offical topomap, E11 is the Fz electrode\n",
    "raw, _ = mne.io.set_eeg_reference(raw, fz_ch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Band-pass filtering from 4 - 30 Hz\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<RawEGI  |  Fanni 20110317 1104002.raw, n_channels x n_times : 131 x 883002 (883.0 sec)>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Filtering ###\n",
    "# picks = mne.pick_types(raw_faster.info, meg=False, eeg=True, eog=False, stim=False, exclude='bads')\n",
    "# raw.plot_psd(area_mode='range', tmax=10.0, fmax=50, picks=picks) # Plot the raw frequency power before filtering\n",
    "\n",
    "high_pass = 4.0\n",
    "low_pass = 30.0\n",
    "raw.filter(high_pass, low_pass)  # high pass over 4 Hz and low pass below 30 Hz\n",
    "# Plot the filtered frequency power\n",
    "# raw.plot_psd(area_mode='range', tmax=10.0, fmax=100, picks=picks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Defining channel types ###\n",
    "stim_chs = ['STI 014', 'resp', 'stim']\n",
    "eog_chs = ['E17', 'E48', 'E119', 'E125', 'E126', 'E127', 'E128'] # TODO These are definitely not EEG channels, however some of them may not be EOG, but EMG (maybe ECG?) channel\n",
    "\n",
    "for ch in eog_chs:\n",
    "    raw.set_channel_types({ch:'eog'})\n",
    "\n",
    "# Channels to include in the analysis (using EEG channels only)\n",
    "eeg_chs = [ch for ch in raw.info['ch_names'] if ch not in stim_chs + eog_chs + fz_ch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Hurst exponent function from 'eegfaster' ###\n",
    "# TODO Check if this is really an accurate implementation\n",
    "#KA nem érhető el ez már Pythonban? pl https://pypi.python.org/pypi/nolds vagy \n",
    "#https://pypi.python.org/pypi/heteromotility/\n",
    "#KA Octave-val lehet ellenőrizni, hogy ugyanat csinálják-e\n",
    "def hurst(x):\n",
    "    \"\"\"FASTER [Nolan2010]_ implementation of the Hurst Exponent.\n",
    "    Parameters\n",
    "    ----------\n",
    "    x : numpy.ndarray\n",
    "        Vector with the data sequence.\n",
    "    Returns\n",
    "    -------\n",
    "    h : float\n",
    "        Computed hurst exponent\n",
    "    #-SPHINX-IGNORE-#\n",
    "    References\n",
    "    ----------\n",
    "    [Nolan2010] H. Nolan, R. Whelan, and R.B. Reilly. Faster: Fully automated\n",
    "    statistical thresholding for eeg artifact rejection. Journal of\n",
    "    Neuroscience Methods, 192(1):152-162, 2010.\n",
    "    #-SPHINX-IGNORE-#\n",
    "    \"\"\"\n",
    "\n",
    "    # Get a copy of the data\n",
    "    x0 = x.copy()\n",
    "    x0_len = len(x)\n",
    "\n",
    "    yvals = np.zeros(x0_len)\n",
    "    xvals = np.zeros(x0_len)\n",
    "    x1 = np.zeros(x0_len)\n",
    "\n",
    "    index = 0\n",
    "    binsize = 1\n",
    "\n",
    "    while x0_len > 4:\n",
    "\n",
    "        y = x0.std()\n",
    "        index += 1\n",
    "        xvals[index] = binsize\n",
    "        yvals[index] = binsize*y\n",
    "\n",
    "        x0_len /= 2\n",
    "        binsize *= 2\n",
    "        for ipoints in xrange(x0_len):\n",
    "            x1[ipoints] = (x0[2*ipoints] + x0[2*ipoints - 1])*0.5\n",
    "\n",
    "        x0 = x1[:x0_len]\n",
    "\n",
    "    # First value is always 0\n",
    "    xvals = xvals[1:index+1]\n",
    "    yvals = yvals[1:index+1]\n",
    "\n",
    "    logx = np.log(xvals)\n",
    "    logy = np.log(yvals)\n",
    "\n",
    "    p2 = np.polyfit(logx, logy, 1)\n",
    "    return p2[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### FASTER ###\n",
    "from multiprocessing import Pool\n",
    "\n",
    "# Arguments\n",
    "crit_z = 3\n",
    "tmin = -0.5  # start of each epoch (500ms before the trigger)\n",
    "tmax = 1.5  # end of each epoch (1500ms after the trigger)\n",
    "baseline = (-0.2, 0)  # means from the first instant to t = - 200ms (as suggested by Nolan et al. (2010))\n",
    "event_id = [1,2,3,4,6,7,8,9]\n",
    "picks_epochs = mne.pick_types(raw.info, meg=False, eeg=True, eog=True, stim=False, exclude=[])\n",
    "\n",
    "# Functions for single parameters (multiprocessing.Pool.map() needs top level functions which are pickleable)\n",
    "def get_channels_corr(ch_pair, data=None):\n",
    "    if not data: data = df\n",
    "    ch_x = ch_pair[0]\n",
    "    ch_y = ch_pair[1]\n",
    "    chs_corr = stats.pearsonr(data[ch_x], data[ch_y])[0]\n",
    "    return (ch_pair, chs_corr)\n",
    "\n",
    "# def get_channel_mcorr(ch, data=None): # TODO This function is extremely slow! \n",
    "#     if not data: data = df\n",
    "#     ch_mcorr = np.mean([stats.pearsonr(data[ch], data[ch2])[0] for ch2 in eeg_chs if ch2 != ch])\n",
    "#     return ch_mcorr                       \n",
    "\n",
    "def get_channel_mcorr(ch, c_dict=None):\n",
    "    if not c_dict: c_dict = corr_dict\n",
    "    ch_mcorr = np.mean([c_dict[key] for key in c_dict.keys() if ch in key])\n",
    "    return ch_mcorr\n",
    "\n",
    "def get_channel_var(ch, data=None):\n",
    "    if not data: data = df\n",
    "    ch_var = np.var(data[ch])\n",
    "    return ch_var\n",
    "                         \n",
    "def get_channel_hurst(ch, data=None):\n",
    "    if not data: data = df\n",
    "    ch_hurst = hurst(np.asarray(data[ch]))\n",
    "    return ch_hurst\n",
    "\n",
    "def get_channel_ampl(ch, data=None):\n",
    "    if not data: data = df\n",
    "    ch_ampl = max(data[ch]) - min(data[ch])\n",
    "    return ch_ampl\n",
    "\n",
    "def get_channel_dev(ch, data=None, mean=None):\n",
    "    if not data: data = df\n",
    "    if not mean: mean = ch_means[ch]\n",
    "    ch_dev = mean - np.mean(data[ch])\n",
    "    return ch_dev\n",
    "\n",
    "# Defining parameter lists for stages\n",
    "params_bad_channels = [get_channel_mcorr, get_channel_var, get_channel_hurst] # Parameters 1, 2, & 3 [Stage I.] (Nolan et al., 2010)\n",
    "params_bad_epochs = [get_channel_ampl, get_channel_dev, get_channel_var] # Parameters 4, 5, & 6 [Stage II.] (Nolan et al., 2010)\n",
    "\n",
    "\n",
    "# # Finding bad channels (Stage I.)\n",
    "# def find_bad_channels(raw, eeg_chs=eeg_chs, crit_z=crit_z):\n",
    "#     # Initializing Pool for multiprocessing\n",
    "#     def initializer(): # Setting up global variables for the child processes\n",
    "#         global df\n",
    "#         df = raw.to_data_frame()\n",
    "#     pool = Pool(None, initializer, ())\n",
    "#     # Computing parameters\n",
    "#     bads = []\n",
    "#     for param in params_bad_channels:\n",
    "#         z_scores = stats.zscore(pool.map(param, eeg_chs))\n",
    "#         # Defining channel outliers\n",
    "#         bads = bads + [eeg_chs[i] for i, z in enumerate(z_scores) if abs(z) > crit_z and eeg_chs[i] not in bads]\n",
    "#     # Closing Pool\n",
    "#     pool.close()\n",
    "#     pool.join()\n",
    "    \n",
    "#     return bads\n",
    "\n",
    "# Finding bad channels (Stage I.)\n",
    "def find_bad_channels(raw, eeg_chs=eeg_chs, crit_z=crit_z):\n",
    "    def get_corr_dict(raw, eeg_chs=eeg_chs):\n",
    "        def get_channel_pairs(eeg_chs=eeg_chs):\n",
    "            ch_pairs = []\n",
    "            for ch_x in eeg_chs:\n",
    "                for ch_y in eeg_chs:\n",
    "                    if ch_x != ch_y and (ch_y, ch_x) not in ch_pairs:\n",
    "                        ch_pairs.append((ch_x, ch_y))\n",
    "            return ch_pairs\n",
    "        ch_pairs = get_channel_pairs()\n",
    "        df_glob = raw.to_data_frame()\n",
    "        def initializer(): # Setting up global variables for the child processes\n",
    "            global df\n",
    "            df = df_glob\n",
    "        pool = Pool(None, initializer, ())\n",
    "        # Get channel correlations\n",
    "        corr_dict = dict(pool.map(get_channels_corr, ch_pairs))\n",
    "        # Closing Pool\n",
    "        pool.close()\n",
    "        pool.join()\n",
    "        return corr_dict, df_glob\n",
    "    # Get correlation table for Parameter 1\n",
    "    corr_dict_glob, df_glob = get_corr_dict(raw)\n",
    "    # Initializing Pool for multiprocessing\n",
    "    def initializer(): # Setting up global variables for the child processes\n",
    "        global df, corr_table\n",
    "        df = df_glob\n",
    "        corr_dict = corr_dict_glob\n",
    "    pool = Pool(None, initializer, ())\n",
    "    # Computing parameters\n",
    "    bads = []\n",
    "    for param in params_bad_channels:\n",
    "        z_scores = stats.zscore(pool.map(param, eeg_chs))\n",
    "        # Defining channel outliers\n",
    "        bads = bads + [eeg_chs[i] for i, z in enumerate(z_scores) if abs(z) > crit_z and eeg_chs[i] not in bads]\n",
    "    # Closing Pool\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    return bads\n",
    "\n",
    "# Finding bad epochs (Stage II.)\n",
    "def find_bad_epochs(raw, events=events, event_id=event_id, tmin=tmin, tmax=tmax, picks=picks_epochs, baseline=baseline):\n",
    "    # Get epochs data\n",
    "    epochs_df = mne.Epochs(raw, events, event_id, tmin, tmax, picks=picks, baseline=baseline, preload=False, proj=True).to_data_frame()\n",
    "    # Get channel means across epochs for Parameter 5\n",
    "    ch_means_glob = pd.DataFrame([np.mean(epochs_df[ch]) for ch in eeg_chs], index=eeg_chs).transpose()\n",
    "    # Iterating over epochs\n",
    "    epochs_vals = np.empty((0,3))\n",
    "    for epoch in set(epochs_df.index.get_level_values(level='epoch')):\n",
    "        # Initializing Pool for multiprocessing\n",
    "        def initializer(): # Setting up global variables for the child processes\n",
    "            global df, ch_means\n",
    "            df = epochs_df.xs(epoch, level='epoch')\n",
    "            ch_means = ch_means_glob # TODO 'ch_means = ch_means' returns with an error\n",
    "        pool = Pool(None, initializer, ())\n",
    "        # Computing parameters for each epoch\n",
    "        vals = []\n",
    "        for param in params_bad_epochs:        \n",
    "            vals.append(np.mean(pool.map(param, eeg_chs)))\n",
    "        epochs_vals = np.vstack([epochs_vals, np.asarray([vals])])\n",
    "        # Closing Pool   \n",
    "        pool.close()\n",
    "        pool.join()\n",
    "    # Defining epoch outliers   \n",
    "    z_scores = stats.zscore(epochs_vals)        \n",
    "    bads = [i for i, z in enumerate(z_scores) if abs(z[0]) > crit_z or abs(z[1]) > crit_z or abs(z[2]) > crit_z]\n",
    "    return bads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting time column to int64...\n",
      "CPU times: user 1.9 s, sys: 676 ms, total: 2.57 s\n",
      "Wall time: 2min 29s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "bad_channels = find_bad_channels(raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['E8', 'E14', 'E21', 'E25', 'E9']\n",
      "Computing interpolation matrix from 116 sensor positions\n",
      "Interpolating 5 sensors\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<RawEGI  |  Fanni 20110317 1104002.raw, n_channels x n_times : 131 x 883002 (883.0 sec)>"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print bad_channels\n",
    "n_interpolated = len(bad_channels)\n",
    "raw.info['bads'] = bad_channels\n",
    "raw.interpolate_bads(reset_bads=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "634 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "0 projection items activated\n",
      "Loading data for 634 events and 2001 original time points ...\n",
      "1 bad epochs dropped\n",
      "Converting time column to int64...\n",
      "CPU times: user 11.9 s, sys: 37.6 s, total: 49.5 s\n",
      "Wall time: 2min 3s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "bad_epochs = find_bad_epochs(raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[174, 188, 271, 372, 527, 531, 532, 631]\n",
      "634 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "0 projection items activated\n",
      "Dropped 8 epochs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Epochs  |  n_events : 626 (good & bad), tmin : -0.5 (s), tmax : 1.5 (s), baseline : (-0.2, 0),\n",
       " '1': 79, '2': 79, '3': 80, '4': 80, '6': 77, '7': 76, '8': 75, '9': 80>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print bad_epochs\n",
    "epochs = mne.Epochs(raw, events, event_id, tmin, tmax, picks=picks_epochs, baseline=baseline, preload=False, proj=True)\n",
    "epochs.drop(bad_epochs)"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
